{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big part about the way I'm going to define things is that variables are functions and functions are variables; there's no difference between the two!\n",
    "\n",
    "What I want is\n",
    "\n",
    "```\n",
    "x, a = Variable()\n",
    "y, z, b, c = Parameter(1), Parameter(1), Parameter(1), Parameter(1)\n",
    "f = ((x + y) * z) / (a * b / c)\n",
    "\n",
    "step_size = 0.05\n",
    "\n",
    "for (x_train, a_train) in train:\n",
    "    parameter_grads = f.get_grad(x_train, a_train)\n",
    "    for (parameter, grad) in parameter_grads:\n",
    "        parameter.value -= step_size * parameter_grad\n",
    "        \n",
    "for (x_test, a_test) in test:\n",
    "    print(f([x_test, a_test]))\n",
    "```\n",
    "\n",
    "Note that I should _not_ have to do a forward pass every single time.  \n",
    "This is poor design for the user; what I should do instead is just have very lazy execution.  \n",
    "Only ever compute anything when the user needs it. I know tf switched to eager; I should read more about\n",
    "how to do eager computation and implement, maybe.\n",
    "\n",
    "Later, training an MLP in the background would look like this:\n",
    "\n",
    "```\n",
    "class MLP:\n",
    "    def __init__(self, input_layer_size, output_layer_size):\n",
    "        x = Variable()\n",
    "        self.input = x\n",
    "        \n",
    "        self.b = Parameter(`some np vector`)\n",
    "        self.W = Parameter(`some np matrix`)\n",
    "        self.f = W * x + b\n",
    "        \n",
    "    def forward_pass(self, input):\n",
    "        return f([input])\n",
    "        \n",
    "    def get_gradients(self, input):\n",
    "        return f.get_grad([input])\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to model\n",
    "\n",
    "$$ f(\\cdot) = \\frac{(x+y+z)*w}{(a+b)*c)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should I design this? We want to get something like\n",
    "\n",
    "```\n",
    "x = Variable(some_number)\n",
    "y = Variable(some_number)\n",
    ".\n",
    ".\n",
    ".\n",
    "c = Variable(some_number)\n",
    "\n",
    "f = ((x + y + z) * w)\n",
    "print(f.grad(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add:\n",
    "    def __init__(self):\n",
    "        self.f = lambda x, y: x + y\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return self.f(x, y)\n",
    "    \n",
    "    def get_grad(self, x, y):\n",
    "        return (1, 1)\n",
    "    \n",
    "class Multiply:\n",
    "    def __init__(self):\n",
    "        self.f = lambda x, y: x*y\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return self.f(x, y)\n",
    "    \n",
    "    def get_grad(self, x, y):\n",
    "        return (y, x)\n",
    "    \n",
    "class Divide:\n",
    "    def __init__(self):\n",
    "        self.f = lambda x, y: x / y\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return self.f(x, y)\n",
    "    \n",
    "    def get_grad(self, x, y):\n",
    "        return (1/y, - x / y**2)\n",
    "    \n",
    "class Exponent:\n",
    "    def __init__(self):\n",
    "        self.f = lambda x, y: x ** y\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        return self.f(x, y)\n",
    "    \n",
    "    def get_grad(self, x, y):\n",
    "        return (y * x ** (y-1), x**y * np.log(x))\n",
    "    \n",
    "add = Add()\n",
    "multiply = Multiply()\n",
    "divide = Divide()\n",
    "exp = Exponent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, parent0, parent1, primitive, eager=True):\n",
    "        self.parent0 = self.ensure_node(parent0)\n",
    "        self.parent1 = self.ensure_node(parent1)\n",
    "        self.primitive = primitive\n",
    "        self.value = None\n",
    "        if eager:\n",
    "            self.value = self.compute()\n",
    "            \n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "        \n",
    "    def ensure_node(self, node):\n",
    "        if isinstance(node, Node) or isinstance(node, Variable):\n",
    "            return node\n",
    "        return Variable(node)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return \n",
    "        \n",
    "    def __add__(self, node):\n",
    "        node = self.ensure_node(node)\n",
    "        return Node(self, node, add)\n",
    "    \n",
    "    def __radd__(self, node):\n",
    "        return Node(node, self, add)\n",
    "    \n",
    "    def __mul__(self, node):\n",
    "        return Node(self, node, multiply)\n",
    "    \n",
    "    def __rmul__(self, node):\n",
    "        return Node(node, self, multiply)\n",
    "    \n",
    "    def __truediv__(self, node):\n",
    "        return Node(self, node, divide)\n",
    "    \n",
    "    def __rtruediv__(self, node):\n",
    "        return Node(node, self, divide)\n",
    "    \n",
    "    def __pow__(self, node):\n",
    "        return Node(self, node, exp)\n",
    "    \n",
    "    def __rpow__(self, node):\n",
    "        return Node(node, self, exp)\n",
    "    \n",
    "    def compute(self):\n",
    "        if self.value is None:\n",
    "            self.value = self.primitive(self.parent0.compute(), self.parent1.compute())\n",
    "            self.compute_gradient()\n",
    "        return self.value\n",
    "    \n",
    "    def compute_gradient(self):\n",
    "        \"\"\"\n",
    "        Computes the gradient with respect to parent0, parent1.\n",
    "        \"\"\"\n",
    "        parent0_grad, parent1_grad = self.primitive.get_grad(self.parent0.compute(), self.parent1.compute())\n",
    "        self.grad_dict = {}\n",
    "        for key in self.parent0.grad_dict:\n",
    "            self.grad_dict[key] = self.parent0.grad_dict[key] * parent0_grad\n",
    "        for key in self.parent1.grad_dict:\n",
    "            self.grad_dict[key] = self.parent1.grad_dict[key] * parent1_grad\n",
    "            \n",
    "    def get_grad(self, var: Variable):\n",
    "        \"\"\"\n",
    "        Returns the gradient of Node with respect to variable, using the \n",
    "            position of the variable passed in in memory.\n",
    "        \"\"\"\n",
    "        return self.grad_dict[id(var)]\n",
    "    \n",
    "    \n",
    "class Variable(Node):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.grad_dict = {id(self): 1}\n",
    "        \n",
    "    def compute(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4512159392: 1}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(4)\n",
    "x.grad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "{4512229416: 1.5, 4512229192: 2.0, 4512230928: -3.0}\n",
      "1.5 -3.0\n"
     ]
    }
   ],
   "source": [
    "x = Variable(4)\n",
    "y = Variable(3)\n",
    "z = Variable(2)\n",
    "w = x * y\n",
    "a = w / z\n",
    "print(a)\n",
    "print(a.grad_dict)\n",
    "print(a.get_grad(x), a.get_grad(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'Variable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-158d01709f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'Variable'"
     ]
    }
   ],
   "source": [
    "x = Variable(2)\n",
    "denominator = 1 + np.e**(-x)\n",
    "f = 1 / (1 + np.e**(x * -1))\n",
    "print(f.compute())\n",
    "print(f.get_grad(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
